{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "YA_TOKEN=os.getenv('YA_TOKEN')\n",
    "\n",
    "# source\n",
    "SOURCE_DISK_ROOT = None # '/Users/oovamoyo/Downloads/MRT'\n",
    "SOURCE_PUBLIC_URL = os.getenv('SOURCE_PUBLIC_URL')\n",
    "\n",
    "# destination\n",
    "DEST_DISK_ROOT = 'Загрузки/MRT_PNGs'\n",
    "ARTIFACTS_DIR = os.path.join(os.getcwd(), \"artifacts\")\n",
    "DEST_LOCAL_ROOT = os.path.join(ARTIFACTS_DIR, DEST_DISK_ROOT)\n",
    "UPLOAD_ARTIFACTS_FROM_LOCAL = False     # False = do NOT upload after conversion\n",
    "DELETE_LOCAL_AFTER_UPLOAD = False       # True = cleanup local artifacts after upload\n",
    "\n",
    "# script settings\n",
    "SKIP_IF_PNG_EXISTS = True\n",
    "PATIENTS_OFFSET = 0\n",
    "PATIENTS_LIMIT = 10 # set None for full run\n",
    "UPLOAD_METADATA_EVERY = None # upload only at the end\n",
    "PATIENTS_WORKERS = os.cpu_count() or 6\n",
    "LOG_TIMING = False\n",
    "\n",
    "# Image conversion settings\n",
    "CLIP_PERCENTILES = (1, 99)\n",
    "OUTPUT_MODE = 'L'\n",
    "PNG_COMPRESS_LEVEL = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b996ab8",
   "metadata": {},
   "source": [
    "## 1) Connect to Yandex Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f22b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token valid: True\n",
      "Disk info (short): {'total_space': 1104880336896, 'used_space': 80625457405}\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "import posixpath\n",
    "import datetime as _dt\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterator, Optional, Dict, Any, List, Tuple\n",
    "import shutil\n",
    "\n",
    "import yadisk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pydicom\n",
    "from pydicom.errors import InvalidDicomError\n",
    "from PIL import Image\n",
    "\n",
    "y_auth = yadisk.Client(token=YA_TOKEN)\n",
    "y_public = yadisk.Client()\n",
    "\n",
    "if y_auth is None:\n",
    "    raise ValueError('set YA_TOKEN in env variable to upload PNGs to Disk')\n",
    "\n",
    "print('Token valid:', y_auth.check_token())\n",
    "info = y_auth.get_disk_info()\n",
    "print('Disk info (short):', {'total_space': info.total_space, 'used_space': info.used_space})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c5994d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yadisk.objects import SyncPublicResourceObject\n",
    "\n",
    "@dataclass\n",
    "class SourceFile:\n",
    "    src_mode: str # disk or public\n",
    "    src_path: str # for disk, absolute disk path, for public, rel path within public folder\n",
    "    rel_path: str # rel path under source root\n",
    "    name: str\n",
    "\n",
    "def maybe_dicom(fname: str, skipdicomdir=True) -> bool:\n",
    "    name_lower = fname.lower()\n",
    "    if name_lower == 'dicomdir' and skipdicomdir:\n",
    "        return False\n",
    "    return fname.startswith('IM') or name_lower.endswith('.dcm') or ('.' not in fname)\n",
    "\n",
    "def ensure_remote_dir(client: yadisk.Client, remote_dir: str):\n",
    "    remote_dir = remote_dir.rstrip('/') or '/'\n",
    "    if remote_dir == '/':\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        client.makedirs(remote_dir)\n",
    "        return\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    parts = [p for p in remote_dir.split('/') if p]\n",
    "    cur = ''\n",
    "    for p in parts:\n",
    "        cur = cur + '/' + p\n",
    "        try:\n",
    "            if not client.exists(cur):\n",
    "                client.mkdir(cur)\n",
    "        except yadisk.exceptions.PathExistsError:\n",
    "            print('cannot make directories for', remote_dir)\n",
    "            pass\n",
    "\n",
    "def list_patient_dirs() -> List[SyncPublicResourceObject]:\n",
    "    out = []\n",
    "    for item in y_public.public_listdir(SOURCE_PUBLIC_URL, sort='name'):\n",
    "        if item.type == \"dir\":\n",
    "            out.append(item)\n",
    "    return out\n",
    "\n",
    "\n",
    "def iter_disk_files(local_root: str) -> Iterator[Tuple[str, List[SourceFile]]]:\n",
    "    \"\"\"Yield (patient_folder, [SourceFile, ...]) from a local folder where top-level dirs are patients.\"\"\"\n",
    "    root_norm = os.path.abspath(local_root)\n",
    "    for patient in os.listdir(root_norm):\n",
    "        patient_path = os.path.join(root_norm, patient)\n",
    "        if not os.path.isdir(patient_path):\n",
    "            continue\n",
    "        files: List[SourceFile] = []\n",
    "        for dirpath, _, filenames in os.walk(patient_path):\n",
    "            for filename in filenames:\n",
    "                abs_path = os.path.join(dirpath, filename)\n",
    "                rel_path = os.path.relpath(abs_path, root_norm).replace(os.sep, \"/\")\n",
    "                if not maybe_dicom(filename):\n",
    "                    continue\n",
    "                files.append(\n",
    "                    SourceFile(src_mode=\"disk\", src_path=abs_path, rel_path=rel_path, name=filename)\n",
    "                )\n",
    "        files.sort(key=lambda s: s.rel_path)\n",
    "        yield patient, files\n",
    "\n",
    "def iter_public_files(client: yadisk.Client, public_url: str) -> Iterator[Tuple[str, List[SourceFile]]]:\n",
    "    \"\"\"Yield (patient_folder, [SourceFile, ...]) from a public folder where top-level dirs are patients.\"\"\"\n",
    "    \n",
    "    for item in client.public_listdir(public_url, path=None, sort='name'):\n",
    "        if item.type != \"dir\":\n",
    "            continue\n",
    "        patient = item.name\n",
    "        files: List[SourceFile] = []\n",
    "        stack = [item.path]\n",
    "        while stack:\n",
    "            cur_rel = stack.pop()\n",
    "            for child in client.public_listdir(public_url, path=cur_rel):\n",
    "                if child.type == \"dir\":\n",
    "                    stack.append(child.path)\n",
    "                elif not maybe_dicom(child.name):\n",
    "                    continue\n",
    "                else:\n",
    "                    files.append(SourceFile(src_mode=\"public\", src_path=child.path, rel_path=child.path, name=child.name))\n",
    "        yield patient, files\n",
    "\n",
    "def iter_source_files() -> Iterator[Tuple[str, List[SourceFile]]]:\n",
    "    \"\"\"Yield (patient_folder, [SourceFile, ...]) using disk or public source.\"\"\"\n",
    "    if SOURCE_DISK_ROOT:\n",
    "        yield from iter_disk_files(SOURCE_DISK_ROOT)\n",
    "    elif SOURCE_PUBLIC_URL:\n",
    "        yield from iter_public_files(y_public, SOURCE_PUBLIC_URL)\n",
    "    else:\n",
    "        raise ValueError(\"Set either SOURCE_DISK_ROOT or SOURCE_PUBLIC_URL\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87873d6b",
   "metadata": {},
   "source": [
    "## 2) DICOM → PNG conversion (2D)\n",
    "\n",
    "Strategy:\n",
    "- Attempt to read the file as DICOM.\n",
    "- If it contains `PixelData`, convert to a normalized 8-bit grayscale PNG.\n",
    "- If it is multi-frame, output one PNG per frame with a suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb746397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_first_number(x: Any) -> Optional[float]:\n",
    "    if x is None:\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(x, (list, tuple)) and len(x) > 0:\n",
    "            return float(x[0])\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def dicom_to_png_bytes(dicom_bytes: bytes) -> Tuple[List[Tuple[str, bytes]], Dict[str, Any]]:\n",
    "    \"\"\"Returns ([(suffix, png_bytes), ...], meta). suffix is '' or '_f000' etc.\"\"\"\n",
    "    bio = io.BytesIO(dicom_bytes)\n",
    "    ds = pydicom.dcmread(bio, force=True)\n",
    "\n",
    "    meta: Dict[str, Any] = {}\n",
    "    meta[\"study_instance_uid\"] = getattr(ds, \"StudyInstanceUID\", None)\n",
    "    meta[\"series_instance_uid\"] = getattr(ds, \"SeriesInstanceUID\", None)\n",
    "    meta[\"sop_instance_uid\"] = getattr(ds, \"SOPInstanceUID\", None)\n",
    "    meta[\"modality\"] = getattr(ds, \"Modality\", None)\n",
    "    meta[\"series_description\"] = getattr(ds, \"SeriesDescription\", None)\n",
    "    meta[\"instance_number\"] = getattr(ds, \"InstanceNumber\", None)\n",
    "    meta[\"acquisition_number\"] = getattr(ds, \"AcquisitionNumber\", None)\n",
    "\n",
    "    meta[\"rows\"] = getattr(ds, \"Rows\", None)\n",
    "    meta[\"cols\"] = getattr(ds, \"Columns\", None)\n",
    "    meta[\"pixel_spacing\"] = list(getattr(ds, \"PixelSpacing\", [])) if hasattr(ds, \"PixelSpacing\") else None\n",
    "    meta[\"slice_thickness\"] = _get_first_number(getattr(ds, \"SliceThickness\", None))\n",
    "    meta[\"slice_location\"] = _get_first_number(getattr(ds, \"SliceLocation\", None))\n",
    "    meta[\"image_position_patient\"] = list(getattr(ds, \"ImagePositionPatient\", [])) if hasattr(ds, \"ImagePositionPatient\") else None\n",
    "\n",
    "    meta[\"bits_allocated\"] = getattr(ds, \"BitsAllocated\", None)\n",
    "    meta[\"photometric_interpretation\"] = getattr(ds, \"PhotometricInterpretation\", None)\n",
    "    meta[\"transfer_syntax_uid\"] = getattr(getattr(ds, \"file_meta\", None), \"TransferSyntaxUID\", None)\n",
    "\n",
    "    if not hasattr(ds, \"PixelData\"):\n",
    "        raise ValueError(\"No PixelData in DICOM\")\n",
    "\n",
    "    arr = ds.pixel_array.astype(np.float32)\n",
    "\n",
    "    slope = _get_first_number(getattr(ds, \"RescaleSlope\", 1.0)) or 1.0\n",
    "    intercept = _get_first_number(getattr(ds, \"RescaleIntercept\", 0.0)) or 0.0\n",
    "    arr = arr * slope + intercept\n",
    "\n",
    "    wc = _get_first_number(getattr(ds, \"WindowCenter\", None))\n",
    "    ww = _get_first_number(getattr(ds, \"WindowWidth\", None))\n",
    "\n",
    "    def normalize_to_u8(x: np.ndarray) -> np.ndarray:\n",
    "        if wc is not None and ww is not None and ww > 0:\n",
    "            lo = wc - ww / 2.0\n",
    "            hi = wc + ww / 2.0\n",
    "            x = np.clip(x, lo, hi)\n",
    "        else:\n",
    "            p_lo, p_hi = np.percentile(x, CLIP_PERCENTILES)\n",
    "            if p_hi <= p_lo:\n",
    "                p_lo, p_hi = float(np.min(x)), float(np.max(x))\n",
    "            x = np.clip(x, p_lo, p_hi)\n",
    "        x = (x - x.min()) / (x.max() - x.min() + 1e-6)\n",
    "        return (x * 255.0).round().astype(np.uint8)\n",
    "\n",
    "    pngs: List[Tuple[str, bytes]] = []\n",
    "    if arr.ndim == 2:\n",
    "        u8 = normalize_to_u8(arr)\n",
    "        im = Image.fromarray(u8, mode=OUTPUT_MODE)\n",
    "        out = io.BytesIO()\n",
    "        im.save(out, format=\"PNG\", compress_level=PNG_COMPRESS_LEVEL)\n",
    "        pngs.append((\"\", out.getvalue()))\n",
    "    elif arr.ndim == 3:\n",
    "        for i in range(arr.shape[0]):\n",
    "            u8 = normalize_to_u8(arr[i])\n",
    "            im = Image.fromarray(u8, mode=OUTPUT_MODE)\n",
    "            out = io.BytesIO()\n",
    "            im.save(out, format=\"PNG\", compress_level=PNG_COMPRESS_LEVEL)\n",
    "            pngs.append((f\"_f{i:03d}\", out.getvalue()))\n",
    "        meta[\"number_of_frames\"] = arr.shape[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported pixel_array ndim={arr.ndim}\")\n",
    "\n",
    "    return pngs, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd798214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download + upload helpers\n",
    "def download_source_bytes(sf: SourceFile) -> bytes:\n",
    "    if sf.src_mode == \"disk\":\n",
    "        with open(sf.src_path, \"rb\") as f:\n",
    "            return f.read()\n",
    "    elif sf.src_mode == \"public\":\n",
    "        out = io.BytesIO()\n",
    "        y_public.download_public(SOURCE_PUBLIC_URL, out, path=sf.src_path)\n",
    "        return out.getvalue()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported src_mode: {sf.src_mode}\")\n",
    "\n",
    "def save_png_locally(png_bytes: bytes, dest_path: str) -> str:\n",
    "    # dest_path is a posix path under DEST_DISK_ROOT that ends with .png\n",
    "    rel_dest = posixpath.relpath(dest_path, DEST_DISK_ROOT.rstrip('/'))\n",
    "    local_path = os.path.join(DEST_LOCAL_ROOT, rel_dest.replace(\"/\", os.sep))\n",
    "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "    with open(local_path, \"wb\") as f:\n",
    "        f.write(png_bytes)\n",
    "    return local_path    \n",
    "\n",
    "def upload_artifacts_from_local() -> None:\n",
    "    \"\"\"\n",
    "    Upload everything under DEST_LOCAL_ROOT to DEST_DISK_ROOT in Yandex.Disk.\n",
    "    This includes metadata.csv and all patient PNG subfolders.\n",
    "    \"\"\"\n",
    "    print(\"uploading artifacts from local root\")\n",
    "    for dirpath, _, filenames in os.walk(DEST_LOCAL_ROOT):\n",
    "        rel_dir = os.path.relpath(dirpath, DEST_LOCAL_ROOT)\n",
    "        # map local dir -> remote dir\n",
    "        remote_dir = DEST_DISK_ROOT.rstrip(\"/\")\n",
    "        if rel_dir not in (\".\", \"\"):\n",
    "            remote_dir = posixpath.join(remote_dir, rel_dir.replace(os.sep, \"/\"))\n",
    "        ensure_remote_dir(y_auth, remote_dir)\n",
    "\n",
    "        for filename in filenames:\n",
    "            local_path = os.path.join(dirpath, filename)\n",
    "            rel_path = os.path.relpath(local_path, DEST_LOCAL_ROOT).replace(os.sep, \"/\")\n",
    "            remote_path = posixpath.join(DEST_DISK_ROOT.rstrip(\"/\"), rel_path)\n",
    "            y_auth.upload(local_path, remote_path, overwrite=True)\n",
    "\n",
    "    print(\"uploading artifacts from local root complete!\")\n",
    "\n",
    "def delete_local_artifacts() -> None:\n",
    "    print('Deleting local artifacts...')\n",
    "    for patient in os.listdir(DEST_LOCAL_ROOT):\n",
    "        patient_path = os.path.join(DEST_LOCAL_ROOT, patient)\n",
    "        if not os.path.isdir(patient_path):\n",
    "            continue\n",
    "        shutil.rmtree(patient_path, ignore_errors=True)\n",
    "    print('Local artifacts deleted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bffe3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata.csv local: /Users/oovamoyo/PycharmProjects/brain_pd_et/artifacts/Загрузки/MRT_PNGs/metadata.csv\n",
      "metadata.csv remote: Загрузки/MRT_PNGs/metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# Metadata CSV writer\n",
    "os.makedirs(DEST_LOCAL_ROOT, exist_ok=True)\n",
    "\n",
    "METADATA_LOCAL_PATH = os.path.join(DEST_LOCAL_ROOT, \"metadata.csv\")\n",
    "METADATA_REMOTE_PATH = posixpath.join(DEST_DISK_ROOT.rstrip('/'), 'metadata.csv')\n",
    "\n",
    "CSV_FIELDS = [\n",
    "    \"processed_at\",\n",
    "    \"status\",\n",
    "    \"error\",\n",
    "    \"src_mode\",\n",
    "    \"src_rel_path\",\n",
    "    \"src_path\",\n",
    "    \"dest_png_path\",\n",
    "    \"frame_index\",\n",
    "    \"patient_folder\",\n",
    "    \"study_instance_uid\",\n",
    "    \"series_instance_uid\",\n",
    "    \"sop_instance_uid\",\n",
    "    \"modality\",\n",
    "    \"series_description\",\n",
    "    \"instance_number\",\n",
    "    \"acquisition_number\",\n",
    "    \"rows\",\n",
    "    \"cols\",\n",
    "    \"pixel_spacing\",\n",
    "    \"slice_thickness\",\n",
    "    \"slice_location\",\n",
    "    \"image_position_patient\",\n",
    "    \"bits_allocated\",\n",
    "    \"photometric_interpretation\",\n",
    "    \"transfer_syntax_uid\",\n",
    "    \"number_of_frames\",\n",
    "]\n",
    "\n",
    "def init_metadata_csv(path: str) -> None:\n",
    "    if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "        return\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=CSV_FIELDS)\n",
    "        w.writeheader()\n",
    "\n",
    "def append_metadata_row(path: str, row: Dict[str, Any]) -> None:\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=CSV_FIELDS)\n",
    "        safe_row = {k: row.get(k, None) for k in CSV_FIELDS}\n",
    "        w.writerow(safe_row)\n",
    "\n",
    "def upload_metadata_csv() -> None:\n",
    "    ensure_remote_dir(y_auth, posixpath.dirname(METADATA_REMOTE_PATH))\n",
    "    print('uploading metadata csv')\n",
    "    y_auth.upload(METADATA_LOCAL_PATH, METADATA_REMOTE_PATH, overwrite=True)\n",
    "    print('uploading metadata csv complete!')\n",
    "\n",
    "init_metadata_csv(METADATA_LOCAL_PATH)\n",
    "print(\"metadata.csv local:\", METADATA_LOCAL_PATH)\n",
    "print(\"metadata.csv remote:\", METADATA_REMOTE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f4a6f",
   "metadata": {},
   "source": [
    "## 3. Main Processing Loop\n",
    "- walks the source tree\n",
    "- converts each DICOM file with PixelData\n",
    "- uploads PNGs\n",
    "- appends a row to metadata.csv for every produced PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a597b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dest_png_path_from_rel(rel_path: str, frame_suffix: str = \"\") -> str:\n",
    "    rel_dir = posixpath.dirname(rel_path)\n",
    "    name = posixpath.basename(rel_path)\n",
    "    base, ext = os.path.splitext(name)\n",
    "    out_name = f\"{base}{frame_suffix}.png\" if ext else f\"{name}{frame_suffix}.png\"\n",
    "    if rel_dir:\n",
    "        return posixpath.join(DEST_DISK_ROOT.rstrip(\"/\"), rel_dir, out_name)\n",
    "    return posixpath.join(DEST_DISK_ROOT.rstrip(\"/\"), out_name)\n",
    "\n",
    "def get_patient_folder_from_rel(rel_path: str) -> str:\n",
    "    return rel_path.split(\"/\", 1)[0] if \"/\" in rel_path else rel_path\n",
    "\n",
    "def process_all(max_files: Optional[int] = None):\n",
    "    ensure_remote_dir(y_auth, DEST_DISK_ROOT)\n",
    "\n",
    "    processed_rows = 0\n",
    "    processed_files = 0\n",
    "    processed_patients = 0\n",
    "    skipped_files = 0\n",
    "    errors = 0\n",
    "    stop_processing = False\n",
    "\n",
    "    patient_groups = list(iter_source_files())\n",
    "    patient_groups = patient_groups[PATIENTS_OFFSET: PATIENTS_OFFSET + PATIENTS_LIMIT] if PATIENTS_LIMIT is not None else patient_groups\n",
    "    for patient_folder, files in tqdm(patient_groups, desc=\"Patients\", total=len(patient_groups)):\n",
    "        for sf in tqdm(files, desc=f\"Files ({patient_folder})\", total=len(files)):\n",
    "            if max_files is not None and processed_files >= max_files:\n",
    "                stop_processing = True\n",
    "                break\n",
    "            try:\n",
    "                if SKIP_IF_PNG_EXISTS:\n",
    "                    expected = dest_png_path_from_rel(sf.rel_path, frame_suffix=\"\")\n",
    "                    if y_auth.exists(expected):\n",
    "                        skipped_files += 1\n",
    "                        continue\n",
    "                t0 = time.time()\n",
    "                dicom_bytes = download_source_bytes(sf)\n",
    "                t1 = time.time()\n",
    "                pngs, dicom_meta = dicom_to_png_bytes(dicom_bytes)\n",
    "                t2 = time.time()\n",
    "                save_time = 0.0\n",
    "                for frame_i, (suffix, png_bytes) in enumerate(pngs):\n",
    "                    dest_path = dest_png_path_from_rel(sf.rel_path, frame_suffix=suffix)\n",
    "                    s = time.time()\n",
    "                    save_png_locally(png_bytes, dest_path)\n",
    "                    save_time += time.time() - s\n",
    "                    row = {\n",
    "                        \"processed_at\": datetime.datetime.now(datetime.UTC).isoformat() + \"Z\",\n",
    "                        \"status\": \"OK\",\n",
    "                        \"error\": None,\n",
    "                        \"src_mode\": sf.src_mode,\n",
    "                        \"src_rel_path\": sf.rel_path,\n",
    "                        \"src_path\": sf.src_path,\n",
    "                        \"dest_png_path\": dest_path,\n",
    "                        \"frame_index\": frame_i if len(pngs) > 1 else 0,\n",
    "                        \"patient_folder\": patient_folder,\n",
    "                        **dicom_meta,\n",
    "                    }\n",
    "                    append_metadata_row(METADATA_LOCAL_PATH, row)\n",
    "                    processed_rows += 1\n",
    "                processed_files += 1\n",
    "                if LOG_TIMING:\n",
    "                    t3 = time.time()\n",
    "                    print(\"[timing] %s: download=%.3fs convert=%.3fs save=%.3fs total=%.3fs\" % (sf.rel_path, t1 - t0, t2 - t1, save_time, t3 - t0))\n",
    "            except InvalidDicomError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                errors += 1\n",
    "                row = {\n",
    "                    \"processed_at\": _dt.datetime.utcnow().isoformat() + \"Z\",\n",
    "                    \"status\": \"ERROR\",\n",
    "                    \"error\": repr(e)[:2000],\n",
    "                    \"src_mode\": sf.src_mode,\n",
    "                    \"src_rel_path\": sf.rel_path,\n",
    "                    \"src_path\": sf.src_path,\n",
    "                    \"dest_png_path\": \"\",\n",
    "                    \"frame_index\": \"\",\n",
    "                    \"patient_folder\": patient_folder,\n",
    "                }\n",
    "                append_metadata_row(METADATA_LOCAL_PATH, row)\n",
    "        # upload_artifacts_via_api() # TOO SLOW\n",
    "        # delete_local_artifacts()\n",
    "        processed_patients += 1\n",
    "        if stop_processing:\n",
    "            break\n",
    "    upload_metadata_csv()\n",
    "    print('Done.')\n",
    "    print({\n",
    "        \"processed_patients\": processed_patients,\n",
    "        \"processed_files\": processed_files,\n",
    "        \"processed_rows\": processed_rows,\n",
    "        \"skipped_files\": skipped_files,\n",
    "        \"errors\": errors,\n",
    "        \"dest_root\": DEST_DISK_ROOT,\n",
    "        \"metadata_remote\": METADATA_REMOTE_PATH,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b4896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files (БП +ЭТ_Валеева фх-3,0 Тл нигросома-1 copy): 100%|██████████| 966/966 [07:45<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading metadata csv\n",
      "uploading metadata csv complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [23:12, 77.35s/it]\n",
      "Patients: 100%|██████████| 1/1 [31:01<00:00, 1861.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "{'processed_patients': 1, 'processed_files': 960, 'processed_rows': 960, 'skipped_files': 6, 'errors': 0, 'dest_root': 'Загрузки/MRT_PNGs', 'metadata_remote': 'Загрузки/MRT_PNGs/metadata.csv'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# process_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246043c",
   "metadata": {},
   "source": [
    "### 3.2 Multi-processing\n",
    "- uses a pool of worker processes to convert patients in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428438bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def _local_png_path(dest_png_path: str) -> str:\n",
    "    # dest_png_path is posix path under DEST_DISK_ROOT, map to DEST_LOCAL_ROOT\n",
    "    rel_dest = posixpath.relpath(dest_png_path, DEST_DISK_ROOT.rstrip(\"/\"))\n",
    "    return os.path.join(DEST_LOCAL_ROOT, rel_dest.replace(\"/\", os.sep))\n",
    "\n",
    "\n",
    "def append_metadata_rows(path: str, rows: List[Dict[str, Any]]) -> None:\n",
    "    # fast batch append (single open)\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=CSV_FIELDS)\n",
    "        for row in rows:\n",
    "            safe_row = {k: row.get(k, None) for k in CSV_FIELDS}\n",
    "            w.writerow(safe_row)\n",
    "\n",
    "\n",
    "def _process_one_patient_worker(args):\n",
    "    \"\"\"\n",
    "    Worker runs one patient end-to-end:\n",
    "      - read dicoms\n",
    "      - convert to png\n",
    "      - save locally\n",
    "      - return metadata rows + stats\n",
    "    \"\"\"\n",
    "    patient_folder, files, max_files = args\n",
    "\n",
    "    processed_files = 0\n",
    "    processed_rows = 0\n",
    "    skipped_files = 0\n",
    "    errors = 0\n",
    "    rows_out: List[Dict[str, Any]] = []\n",
    "\n",
    "    for i, sf in enumerate(files):\n",
    "        if max_files is not None and i >= max_files:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            dicom_bytes = download_source_bytes(sf)\n",
    "            pngs, dicom_meta = dicom_to_png_bytes(dicom_bytes)\n",
    "\n",
    "            for frame_i, (suffix, png_bytes) in enumerate(pngs):\n",
    "                dest_path = dest_png_path_from_rel(sf.rel_path, frame_suffix=suffix)\n",
    "\n",
    "                if SKIP_IF_PNG_EXISTS:\n",
    "                    local_png = _local_png_path(dest_path)\n",
    "                    if os.path.exists(local_png):\n",
    "                        skipped_files += 1\n",
    "                        continue\n",
    "\n",
    "                save_png_locally(png_bytes, dest_path)\n",
    "\n",
    "                row = {\n",
    "                    \"processed_at\": _dt.datetime.utcnow().isoformat() + \"Z\",\n",
    "                    \"status\": \"OK\",\n",
    "                    \"error\": None,\n",
    "                    \"src_mode\": sf.src_mode,\n",
    "                    \"src_rel_path\": sf.rel_path,\n",
    "                    \"src_path\": sf.src_path,\n",
    "                    \"dest_png_path\": dest_path,\n",
    "                    \"frame_index\": frame_i if len(pngs) > 1 else 0,\n",
    "                    \"patient_folder\": patient_folder,\n",
    "                    **dicom_meta,\n",
    "                }\n",
    "                rows_out.append(row)\n",
    "                processed_rows += 1\n",
    "\n",
    "            processed_files += 1\n",
    "\n",
    "        except InvalidDicomError:\n",
    "            # treat as non-fatal skip (match your existing behavior if you do this)\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            rows_out.append({\n",
    "                \"processed_at\": _dt.datetime.utcnow().isoformat() + \"Z\",\n",
    "                \"status\": \"ERROR\",\n",
    "                \"error\": repr(e)[:2000],\n",
    "                \"src_mode\": sf.src_mode,\n",
    "                \"src_rel_path\": sf.rel_path,\n",
    "                \"src_path\": sf.src_path,\n",
    "                \"dest_png_path\": \"\",\n",
    "                \"frame_index\": \"\",\n",
    "                \"patient_folder\": patient_folder,\n",
    "            })\n",
    "\n",
    "    return (patient_folder, processed_files, processed_rows, skipped_files, errors, rows_out)\n",
    "\n",
    "\n",
    "def process_parallel(max_files: Optional[int] = None, workers: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Parallel version of process_all:\n",
    "      - respects PATIENTS_OFFSET / PATIENTS_LIMIT\n",
    "      - processes patients in separate processes\n",
    "      - writes metadata.csv in main process (safe)\n",
    "    \"\"\"\n",
    "    workers = workers or PATIENTS_WORKERS\n",
    "\n",
    "    # Collect patients respecting offset/limit\n",
    "    patients = list(iter_source_files())\n",
    "    if PATIENTS_OFFSET:\n",
    "        patients = patients[PATIENTS_OFFSET:]\n",
    "    if PATIENTS_LIMIT is not None:\n",
    "        patients = patients[:PATIENTS_LIMIT]\n",
    "\n",
    "    # Prepare args\n",
    "    args = [(patient, files, max_files) for patient, files in patients]\n",
    "\n",
    "    processed_files = 0\n",
    "    processed_rows = 0\n",
    "    skipped_files = 0\n",
    "    errors = 0\n",
    "\n",
    "    # Prefer fork on mac/linux for notebook compatibility\n",
    "    try:\n",
    "        ctx = mp.get_context(\"fork\")\n",
    "    except Exception:\n",
    "        ctx = mp.get_context()  # fallback\n",
    "\n",
    "    for_a_patient = len(args)\n",
    "\n",
    "    with ctx.Pool(processes=workers) as pool:\n",
    "        for (patient_folder,\n",
    "             p_files, p_rows, p_skipped, p_errors, rows_out) in tqdm(\n",
    "                pool.imap_unordered(_process_one_patient_worker, args),\n",
    "                total=for_a_patient\n",
    "        ):\n",
    "            print(f\"processed patient: {patient_folder}\")\n",
    "            append_metadata_rows(METADATA_LOCAL_PATH, rows_out)\n",
    "\n",
    "            processed_files += p_files\n",
    "            processed_rows += p_rows\n",
    "            skipped_files += p_skipped\n",
    "            errors += p_errors\n",
    "\n",
    "    # upload artifacts optionally (deferred by default)\n",
    "    if UPLOAD_ARTIFACTS_FROM_LOCAL:\n",
    "        upload_artifacts_from_local()\n",
    "        upload_metadata_csv()\n",
    "        if DELETE_LOCAL_AFTER_UPLOAD:\n",
    "            delete_local_artifacts()\n",
    "\n",
    "    print(\"Done.\")\n",
    "    print({\n",
    "        \"processed_files\": processed_files,\n",
    "        \"processed_rows\": processed_rows,\n",
    "        \"skipped_files\": skipped_files,\n",
    "        \"errors\": errors,\n",
    "        \"dest_root\": DEST_DISK_ROOT,\n",
    "        \"metadata_remote\": METADATA_REMOTE_PATH,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Usage\n",
    "process_parallel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
