{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edaa68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "YA_TOKEN=os.getenv('YA_TOKEN')\n",
    "\n",
    "# source\n",
    "SOURCE_DISK_ROOT = None # '/Users/oovamoyo/Downloads/MRT'\n",
    "SOURCE_PUBLIC_URL = os.getenv('SOURCE_PUBLIC_URL')\n",
    "\n",
    "# destination\n",
    "DEST_DISK_ROOT = 'Загрузки/MRT_PNGs'\n",
    "ARTIFACTS_DIR = os.path.join(os.getcwd(), \"artifacts\")\n",
    "DEST_LOCAL_ROOT = os.path.join(ARTIFACTS_DIR, DEST_DISK_ROOT)\n",
    "UPLOAD_ARTIFACTS_FROM_LOCAL = False     # False = do NOT upload after conversion\n",
    "DELETE_LOCAL_AFTER_UPLOAD = False       # True = cleanup local artifacts after upload\n",
    "\n",
    "# script settings\n",
    "SKIP_IF_PNG_EXISTS = True\n",
    "PATIENTS_OFFSET = 0\n",
    "PATIENTS_LIMIT = 10 # set None for full run\n",
    "UPLOAD_METADATA_EVERY = None # upload only at the end\n",
    "PATIENTS_WORKERS = 6\n",
    "LOG_TIMING = False\n",
    "\n",
    "# Image conversion settings\n",
    "CLIP_PERCENTILES = (1, 99)\n",
    "OUTPUT_MODE = 'L'\n",
    "PNG_COMPRESS_LEVEL = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b996ab8",
   "metadata": {},
   "source": [
    "## 1) Connect to Yandex Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f22b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token valid: True\n",
      "Disk info (short): {'total_space': 1104880336896, 'used_space': 80725741646}\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "import tempfile\n",
    "import posixpath\n",
    "import datetime as _dt\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterator, Optional, Dict, Any, List, Tuple\n",
    "import shutil\n",
    "\n",
    "import yadisk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pydicom\n",
    "from pydicom.errors import InvalidDicomError\n",
    "from PIL import Image\n",
    "\n",
    "y_auth = yadisk.Client(token=YA_TOKEN)\n",
    "y_public = yadisk.Client()\n",
    "\n",
    "if y_auth is None:\n",
    "    raise ValueError('set YA_TOKEN in env variable to upload PNGs to Disk')\n",
    "\n",
    "print('Token valid:', y_auth.check_token())\n",
    "info = y_auth.get_disk_info()\n",
    "print('Disk info (short):', {'total_space': info.total_space, 'used_space': info.used_space})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c5994d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yadisk.objects import SyncPublicResourceObject\n",
    "\n",
    "@dataclass\n",
    "class SourceFile:\n",
    "    src_mode: str # disk or public\n",
    "    src_path: str # for disk, absolute disk path, for public, rel path within public folder\n",
    "    rel_path: str # rel path under source root\n",
    "    name: str\n",
    "\n",
    "def maybe_dicom(fname: str, skipdicomdir=True) -> bool:\n",
    "    name_lower = fname.lower()\n",
    "    if name_lower == 'dicomdir' and skipdicomdir:\n",
    "        return False\n",
    "    return fname.startswith('IM') or name_lower.endswith('.dcm') or ('.' not in fname)\n",
    "\n",
    "def ensure_remote_dir(client: yadisk.Client, remote_dir: str):\n",
    "    remote_dir = remote_dir.rstrip('/') or '/'\n",
    "    if remote_dir == '/':\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        client.makedirs(remote_dir)\n",
    "        return\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    parts = [p for p in remote_dir.split('/') if p]\n",
    "    cur = ''\n",
    "    for p in parts:\n",
    "        cur = cur + '/' + p\n",
    "        try:\n",
    "            if not client.exists(cur):\n",
    "                client.mkdir(cur)\n",
    "        except yadisk.exceptions.PathExistsError:\n",
    "            print('cannot make directories for', remote_dir)\n",
    "            pass\n",
    "\n",
    "def list_patient_dirs() -> List[SyncPublicResourceObject]:\n",
    "    out = []\n",
    "    for item in y_public.public_listdir(SOURCE_PUBLIC_URL, sort='name'):\n",
    "        if item.type == \"dir\":\n",
    "            out.append(item)\n",
    "    return out\n",
    "\n",
    "\n",
    "def iter_disk_files(local_root: str) -> Iterator[Tuple[str, List[SourceFile]]]:\n",
    "    \"\"\"Yield (patient_folder, [SourceFile, ...]) from a local folder where top-level dirs are patients.\"\"\"\n",
    "    root_norm = os.path.abspath(local_root)\n",
    "    for patient in os.listdir(root_norm):\n",
    "        patient_path = os.path.join(root_norm, patient)\n",
    "        if not os.path.isdir(patient_path):\n",
    "            continue\n",
    "        files: List[SourceFile] = []\n",
    "        for dirpath, _, filenames in os.walk(patient_path):\n",
    "            for filename in filenames:\n",
    "                abs_path = os.path.join(dirpath, filename)\n",
    "                rel_path = os.path.relpath(abs_path, root_norm).replace(os.sep, \"/\")\n",
    "                if not maybe_dicom(filename):\n",
    "                    continue\n",
    "                files.append(\n",
    "                    SourceFile(src_mode=\"disk\", src_path=abs_path, rel_path=rel_path, name=filename)\n",
    "                )\n",
    "        files.sort(key=lambda s: s.rel_path)\n",
    "        yield patient, files\n",
    "\n",
    "def iter_public_files(client: yadisk.Client, public_url: str) -> Iterator[Tuple[str, List[SourceFile]]]:\n",
    "    \"\"\"Yield (patient_folder, [SourceFile, ...]) from a public folder where top-level dirs are patients.\"\"\"\n",
    "    \n",
    "    for item in client.public_listdir(public_url, path=None, sort='name'):\n",
    "        if item.type != \"dir\":\n",
    "            continue\n",
    "        patient = item.name\n",
    "        files: List[SourceFile] = []\n",
    "        stack = [item.path]\n",
    "        while stack:\n",
    "            cur_rel = stack.pop()\n",
    "            for child in client.public_listdir(public_url, path=cur_rel):\n",
    "                if child.type == \"dir\":\n",
    "                    stack.append(child.path)\n",
    "                elif not maybe_dicom(child.name):\n",
    "                    continue\n",
    "                else:\n",
    "                    files.append(SourceFile(src_mode=\"public\", src_path=child.path, rel_path=child.path, name=child.name))\n",
    "        yield patient, files\n",
    "\n",
    "def iter_source_files() -> Iterator[Tuple[str, List[SourceFile]]]:\n",
    "    \"\"\"Yield (patient_folder, [SourceFile, ...]) using disk or public source.\"\"\"\n",
    "    if SOURCE_DISK_ROOT:\n",
    "        yield from iter_disk_files(SOURCE_DISK_ROOT)\n",
    "    elif SOURCE_PUBLIC_URL:\n",
    "        yield from iter_public_files(y_public, SOURCE_PUBLIC_URL)\n",
    "    else:\n",
    "        raise ValueError(\"Set either SOURCE_DISK_ROOT or SOURCE_PUBLIC_URL\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87873d6b",
   "metadata": {},
   "source": [
    "## 2) DICOM → PNG conversion (2D)\n",
    "\n",
    "Strategy:\n",
    "- Attempt to read the file as DICOM.\n",
    "- If it contains `PixelData`, convert to a normalized 8-bit grayscale PNG.\n",
    "- If it is multi-frame, output one PNG per frame with a suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb746397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_first_number(x: Any) -> Optional[float]:\n",
    "    if x is None:\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(x, (list, tuple)) and len(x) > 0:\n",
    "            return float(x[0])\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def dicom_to_png_bytes(dicom_bytes: bytes) -> Tuple[List[Tuple[str, bytes]], Dict[str, Any]]:\n",
    "    \"\"\"Returns ([(suffix, png_bytes), ...], meta). suffix is '' or '_f000' etc.\"\"\"\n",
    "    bio = io.BytesIO(dicom_bytes)\n",
    "    ds = pydicom.dcmread(bio, force=True)\n",
    "\n",
    "    meta: Dict[str, Any] = {}\n",
    "    meta[\"study_instance_uid\"] = getattr(ds, \"StudyInstanceUID\", None)\n",
    "    meta[\"series_instance_uid\"] = getattr(ds, \"SeriesInstanceUID\", None)\n",
    "    meta[\"sop_instance_uid\"] = getattr(ds, \"SOPInstanceUID\", None)\n",
    "    meta[\"modality\"] = getattr(ds, \"Modality\", None)\n",
    "    meta[\"series_description\"] = getattr(ds, \"SeriesDescription\", None)\n",
    "    meta[\"instance_number\"] = getattr(ds, \"InstanceNumber\", None)\n",
    "    meta[\"acquisition_number\"] = getattr(ds, \"AcquisitionNumber\", None)\n",
    "\n",
    "    meta[\"rows\"] = getattr(ds, \"Rows\", None)\n",
    "    meta[\"cols\"] = getattr(ds, \"Columns\", None)\n",
    "    meta[\"pixel_spacing\"] = list(getattr(ds, \"PixelSpacing\", [])) if hasattr(ds, \"PixelSpacing\") else None\n",
    "    meta[\"slice_thickness\"] = _get_first_number(getattr(ds, \"SliceThickness\", None))\n",
    "    meta[\"slice_location\"] = _get_first_number(getattr(ds, \"SliceLocation\", None))\n",
    "    meta[\"image_position_patient\"] = list(getattr(ds, \"ImagePositionPatient\", [])) if hasattr(ds, \"ImagePositionPatient\") else None\n",
    "\n",
    "    meta[\"bits_allocated\"] = getattr(ds, \"BitsAllocated\", None)\n",
    "    meta[\"photometric_interpretation\"] = getattr(ds, \"PhotometricInterpretation\", None)\n",
    "    meta[\"transfer_syntax_uid\"] = getattr(getattr(ds, \"file_meta\", None), \"TransferSyntaxUID\", None)\n",
    "\n",
    "    if not hasattr(ds, \"PixelData\"):\n",
    "        raise ValueError(\"No PixelData in DICOM\")\n",
    "\n",
    "    arr = ds.pixel_array.astype(np.float32)\n",
    "\n",
    "    slope = _get_first_number(getattr(ds, \"RescaleSlope\", 1.0)) or 1.0\n",
    "    intercept = _get_first_number(getattr(ds, \"RescaleIntercept\", 0.0)) or 0.0\n",
    "    arr = arr * slope + intercept\n",
    "\n",
    "    wc = _get_first_number(getattr(ds, \"WindowCenter\", None))\n",
    "    ww = _get_first_number(getattr(ds, \"WindowWidth\", None))\n",
    "\n",
    "    def normalize_to_u8(x: np.ndarray) -> np.ndarray:\n",
    "        if wc is not None and ww is not None and ww > 0:\n",
    "            lo = wc - ww / 2.0\n",
    "            hi = wc + ww / 2.0\n",
    "            x = np.clip(x, lo, hi)\n",
    "        else:\n",
    "            p_lo, p_hi = np.percentile(x, CLIP_PERCENTILES)\n",
    "            if p_hi <= p_lo:\n",
    "                p_lo, p_hi = float(np.min(x)), float(np.max(x))\n",
    "            x = np.clip(x, p_lo, p_hi)\n",
    "        x = (x - x.min()) / (x.max() - x.min() + 1e-6)\n",
    "        return (x * 255.0).round().astype(np.uint8)\n",
    "\n",
    "    pngs: List[Tuple[str, bytes]] = []\n",
    "    if arr.ndim == 2:\n",
    "        u8 = normalize_to_u8(arr)\n",
    "        im = Image.fromarray(u8, mode=OUTPUT_MODE)\n",
    "        out = io.BytesIO()\n",
    "        im.save(out, format=\"PNG\", compress_level=PNG_COMPRESS_LEVEL)\n",
    "        pngs.append((\"\", out.getvalue()))\n",
    "    elif arr.ndim == 3:\n",
    "        for i in range(arr.shape[0]):\n",
    "            u8 = normalize_to_u8(arr[i])\n",
    "            im = Image.fromarray(u8, mode=OUTPUT_MODE)\n",
    "            out = io.BytesIO()\n",
    "            im.save(out, format=\"PNG\", compress_level=PNG_COMPRESS_LEVEL)\n",
    "            pngs.append((f\"_f{i:03d}\", out.getvalue()))\n",
    "        meta[\"number_of_frames\"] = arr.shape[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported pixel_array ndim={arr.ndim}\")\n",
    "\n",
    "    return pngs, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd798214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download + upload helpers\n",
    "def download_source_bytes(sf: SourceFile, pc: yadisk.Client = y_public) -> bytes:\n",
    "    if sf.src_mode == \"disk\":\n",
    "        with open(sf.src_path, \"rb\") as f:\n",
    "            return f.read()\n",
    "    elif sf.src_mode == \"public\":\n",
    "        out = io.BytesIO()\n",
    "        pc.download_public(SOURCE_PUBLIC_URL, out, path=sf.src_path)\n",
    "        return out.getvalue()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported src_mode: {sf.src_mode}\")\n",
    "\n",
    "def save_png_locally(png_bytes: bytes, dest_path: str) -> str:\n",
    "    # dest_path is a posix path under DEST_DISK_ROOT that ends with .png\n",
    "    rel_dest = posixpath.relpath(dest_path, DEST_DISK_ROOT.rstrip('/'))\n",
    "    local_path = os.path.join(DEST_LOCAL_ROOT, rel_dest.replace(\"/\", os.sep))\n",
    "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "    with open(local_path, \"wb\") as f:\n",
    "        f.write(png_bytes)\n",
    "    return local_path    \n",
    "\n",
    "def upload_artifacts_from_local() -> None:\n",
    "    \"\"\"\n",
    "    Upload everything under DEST_LOCAL_ROOT to DEST_DISK_ROOT in Yandex.Disk.\n",
    "    This includes metadata.csv and all patient PNG subfolders.\n",
    "    \"\"\"\n",
    "    print(\"uploading artifacts from local root\")\n",
    "    for dirpath, _, filenames in os.walk(DEST_LOCAL_ROOT):\n",
    "        rel_dir = os.path.relpath(dirpath, DEST_LOCAL_ROOT)\n",
    "        # map local dir -> remote dir\n",
    "        remote_dir = DEST_DISK_ROOT.rstrip(\"/\")\n",
    "        if rel_dir not in (\".\", \"\"):\n",
    "            remote_dir = posixpath.join(remote_dir, rel_dir.replace(os.sep, \"/\"))\n",
    "        ensure_remote_dir(y_auth, remote_dir)\n",
    "\n",
    "        for filename in filenames:\n",
    "            local_path = os.path.join(dirpath, filename)\n",
    "            rel_path = os.path.relpath(local_path, DEST_LOCAL_ROOT).replace(os.sep, \"/\")\n",
    "            remote_path = posixpath.join(DEST_DISK_ROOT.rstrip(\"/\"), rel_path)\n",
    "            y_auth.upload(local_path, remote_path, overwrite=True)\n",
    "\n",
    "    print(\"uploading artifacts from local root complete!\")\n",
    "\n",
    "def delete_local_artifacts() -> None:\n",
    "    print('Deleting local artifacts...')\n",
    "    for patient in os.listdir(DEST_LOCAL_ROOT):\n",
    "        patient_path = os.path.join(DEST_LOCAL_ROOT, patient)\n",
    "        if not os.path.isdir(patient_path):\n",
    "            continue\n",
    "        shutil.rmtree(patient_path, ignore_errors=True)\n",
    "    print('Local artifacts deleted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bffe3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata.csv local: /Users/oovamoyo/PycharmProjects/brain_pd_et/artifacts/Загрузки/MRT_PNGs/metadata.csv\n",
      "metadata.csv remote: Загрузки/MRT_PNGs/metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# Metadata CSV writer\n",
    "os.makedirs(DEST_LOCAL_ROOT, exist_ok=True)\n",
    "\n",
    "METADATA_LOCAL_PATH = os.path.join(DEST_LOCAL_ROOT, \"metadata.csv\")\n",
    "METADATA_REMOTE_PATH = posixpath.join(DEST_DISK_ROOT.rstrip('/'), 'metadata.csv')\n",
    "\n",
    "CSV_FIELDS = [\n",
    "    \"processed_at\",\n",
    "    \"status\",\n",
    "    \"error\",\n",
    "    \"src_mode\",\n",
    "    \"src_rel_path\",\n",
    "    \"src_path\",\n",
    "    \"dest_png_path\",\n",
    "    \"frame_index\",\n",
    "    \"patient_folder\",\n",
    "    \"study_instance_uid\",\n",
    "    \"series_instance_uid\",\n",
    "    \"sop_instance_uid\",\n",
    "    \"modality\",\n",
    "    \"series_description\",\n",
    "    \"instance_number\",\n",
    "    \"acquisition_number\",\n",
    "    \"rows\",\n",
    "    \"cols\",\n",
    "    \"pixel_spacing\",\n",
    "    \"slice_thickness\",\n",
    "    \"slice_location\",\n",
    "    \"image_position_patient\",\n",
    "    \"bits_allocated\",\n",
    "    \"photometric_interpretation\",\n",
    "    \"transfer_syntax_uid\",\n",
    "    \"number_of_frames\",\n",
    "]\n",
    "\n",
    "def init_metadata_csv(path: str) -> None:\n",
    "    if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "        return\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=CSV_FIELDS)\n",
    "        w.writeheader()\n",
    "\n",
    "def append_metadata_row(path: str, row: Dict[str, Any]) -> None:\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=CSV_FIELDS)\n",
    "        safe_row = {k: row.get(k, None) for k in CSV_FIELDS}\n",
    "        w.writerow(safe_row)\n",
    "\n",
    "def upload_metadata_csv() -> None:\n",
    "    merge_metadata_csv()\n",
    "\n",
    "def merge_metadata_csv() -> None:\n",
    "    ensure_remote_dir(y_auth, posixpath.dirname(METADATA_REMOTE_PATH))\n",
    "    tmp_fd, tmp_path = tempfile.mkstemp(suffix=\"_metadata.csv\")\n",
    "    os.close(tmp_fd)\n",
    "    try:\n",
    "        if y_auth.exists(METADATA_REMOTE_PATH):\n",
    "            try:\n",
    "                with open(tmp_path, \"wb\") as f:\n",
    "                    y_auth.download(METADATA_REMOTE_PATH, f)\n",
    "                df_remote = pd.read_csv(tmp_path)\n",
    "            except Exception:\n",
    "                df_remote = pd.DataFrame(columns=CSV_FIELDS)\n",
    "        else:\n",
    "            df_remote = pd.DataFrame(columns=CSV_FIELDS)\n",
    "        try:\n",
    "            df_local = pd.read_csv(METADATA_LOCAL_PATH)\n",
    "        except Exception:\n",
    "            df_local = pd.DataFrame(columns=CSV_FIELDS)\n",
    "        merged = pd.concat([df_remote, df_local], ignore_index=True)\n",
    "        merged = merged.drop_duplicates()\n",
    "        merged = merged.reindex(columns=CSV_FIELDS)\n",
    "        merged.to_csv(METADATA_LOCAL_PATH, index=False)\n",
    "        y_auth.upload(METADATA_LOCAL_PATH, METADATA_REMOTE_PATH, overwrite=True)\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(tmp_path)\n",
    "        except Exception:\n",
    "            pass\n",
    "    print('uploaded merged metadata csv')\n",
    "init_metadata_csv(METADATA_LOCAL_PATH)\n",
    "print(\"metadata.csv local:\", METADATA_LOCAL_PATH)\n",
    "print(\"metadata.csv remote:\", METADATA_REMOTE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f4a6f",
   "metadata": {},
   "source": [
    "## 3. Main Processing Loop\n",
    "- walks the source tree\n",
    "- converts each DICOM file with PixelData\n",
    "- uploads PNGs\n",
    "- appends a row to metadata.csv for every produced PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a597b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dest_png_path_from_rel(rel_path: str, frame_suffix: str = \"\") -> str:\n",
    "    rel_dir = posixpath.dirname(rel_path)\n",
    "    name = posixpath.basename(rel_path)\n",
    "    base, ext = os.path.splitext(name)\n",
    "    out_name = f\"{base}{frame_suffix}.png\" if ext else f\"{name}{frame_suffix}.png\"\n",
    "    if rel_dir:\n",
    "        return posixpath.join(DEST_DISK_ROOT.rstrip(\"/\"), rel_dir, out_name)\n",
    "    return posixpath.join(DEST_DISK_ROOT.rstrip(\"/\"), out_name)\n",
    "\n",
    "def get_patient_folder_from_rel(rel_path: str) -> str:\n",
    "    return rel_path.split(\"/\", 1)[0] if \"/\" in rel_path else rel_path\n",
    "\n",
    "def process_all(max_files: Optional[int] = None):\n",
    "    ensure_remote_dir(y_auth, DEST_DISK_ROOT)\n",
    "\n",
    "    processed_rows = 0\n",
    "    processed_files = 0\n",
    "    processed_patients = 0\n",
    "    skipped_files = 0\n",
    "    errors = 0\n",
    "    stop_processing = False\n",
    "\n",
    "    patient_groups = list(iter_source_files())\n",
    "    patient_groups = patient_groups[PATIENTS_OFFSET: PATIENTS_OFFSET + PATIENTS_LIMIT] if PATIENTS_LIMIT is not None else patient_groups\n",
    "    for patient_folder, files in tqdm(patient_groups, desc=\"Patients\", total=len(patient_groups)):\n",
    "        for sf in tqdm(files, desc=f\"Files ({patient_folder})\", total=len(files)):\n",
    "            if max_files is not None and processed_files >= max_files:\n",
    "                stop_processing = True\n",
    "                break\n",
    "            try:\n",
    "                if SKIP_IF_PNG_EXISTS:\n",
    "                    expected = dest_png_path_from_rel(sf.rel_path, frame_suffix=\"\")\n",
    "                    if y_auth.exists(expected):\n",
    "                        skipped_files += 1\n",
    "                        continue\n",
    "                t0 = time.time()\n",
    "                dicom_bytes = download_source_bytes(sf)\n",
    "                t1 = time.time()\n",
    "                pngs, dicom_meta = dicom_to_png_bytes(dicom_bytes)\n",
    "                t2 = time.time()\n",
    "                save_time = 0.0\n",
    "                for frame_i, (suffix, png_bytes) in enumerate(pngs):\n",
    "                    dest_path = dest_png_path_from_rel(sf.rel_path, frame_suffix=suffix)\n",
    "                    s = time.time()\n",
    "                    save_png_locally(png_bytes, dest_path)\n",
    "                    save_time += time.time() - s\n",
    "                    row = {\n",
    "                        \"processed_at\": datetime.datetime.now(datetime.UTC).isoformat() + \"Z\",\n",
    "                        \"status\": \"OK\",\n",
    "                        \"error\": None,\n",
    "                        \"src_mode\": sf.src_mode,\n",
    "                        \"src_rel_path\": sf.rel_path,\n",
    "                        \"src_path\": sf.src_path,\n",
    "                        \"dest_png_path\": dest_path,\n",
    "                        \"frame_index\": frame_i if len(pngs) > 1 else 0,\n",
    "                        \"patient_folder\": patient_folder,\n",
    "                        **dicom_meta,\n",
    "                    }\n",
    "                    append_metadata_row(METADATA_LOCAL_PATH, row)\n",
    "                    processed_rows += 1\n",
    "                processed_files += 1\n",
    "                if LOG_TIMING:\n",
    "                    t3 = time.time()\n",
    "                    print(\"[timing] %s: download=%.3fs convert=%.3fs save=%.3fs total=%.3fs\" % (sf.rel_path, t1 - t0, t2 - t1, save_time, t3 - t0))\n",
    "            except InvalidDicomError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                errors += 1\n",
    "                row = {\n",
    "                    \"processed_at\": _dt.datetime.utcnow().isoformat() + \"Z\",\n",
    "                    \"status\": \"ERROR\",\n",
    "                    \"error\": repr(e)[:2000],\n",
    "                    \"src_mode\": sf.src_mode,\n",
    "                    \"src_rel_path\": sf.rel_path,\n",
    "                    \"src_path\": sf.src_path,\n",
    "                    \"dest_png_path\": \"\",\n",
    "                    \"frame_index\": \"\",\n",
    "                    \"patient_folder\": patient_folder,\n",
    "                }\n",
    "                append_metadata_row(METADATA_LOCAL_PATH, row)\n",
    "        processed_patients += 1\n",
    "        if stop_processing:\n",
    "            break\n",
    "    upload_metadata_csv()\n",
    "    print('Done.')\n",
    "    print({\n",
    "        \"processed_patients\": processed_patients,\n",
    "        \"processed_files\": processed_files,\n",
    "        \"processed_rows\": processed_rows,\n",
    "        \"skipped_files\": skipped_files,\n",
    "        \"errors\": errors,\n",
    "        \"dest_root\": DEST_DISK_ROOT,\n",
    "        \"metadata_remote\": METADATA_REMOTE_PATH,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c62b4896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246043c",
   "metadata": {},
   "source": [
    "### 3.2 Multi-processing\n",
    "- uses a pool of worker processes to convert patients in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9cfaacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pack_source_file(sf: SourceFile) -> dict:\n",
    "    # Only primitives; pickle-safe\n",
    "    return {\n",
    "        \"src_mode\": sf.src_mode,\n",
    "        \"src_path\": sf.src_path,\n",
    "        \"rel_path\": sf.rel_path,\n",
    "        \"name\": sf.name,\n",
    "    }\n",
    "\n",
    "def _unpack_source_file(d: dict) -> SourceFile:\n",
    "    return SourceFile(\n",
    "        src_mode=d[\"src_mode\"],\n",
    "        src_path=d[\"src_path\"],\n",
    "        rel_path=d[\"rel_path\"],\n",
    "        name=d[\"name\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "428438bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loky import get_reusable_executor\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "\n",
    "def _local_png_path(dest_png_path: str) -> str:\n",
    "    # dest_png_path is posix path under DEST_DISK_ROOT, map to DEST_LOCAL_ROOT\n",
    "    rel_dest = posixpath.relpath(dest_png_path, DEST_DISK_ROOT.rstrip(\"/\"))\n",
    "    return os.path.join(DEST_LOCAL_ROOT, rel_dest.replace(\"/\", os.sep))\n",
    "\n",
    "\n",
    "def append_metadata_rows(path: str, rows: List[Dict[str, Any]]) -> None:\n",
    "    # fast batch append (single open)\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=CSV_FIELDS)\n",
    "        for row in rows:\n",
    "            safe_row = {k: row.get(k, None) for k in CSV_FIELDS}\n",
    "            w.writerow(safe_row)\n",
    "\n",
    "\n",
    "def _process_one_patient_worker(args):\n",
    "    \"\"\"\n",
    "    Worker runs one patient end-to-end:\n",
    "      - read dicoms\n",
    "      - convert to png\n",
    "      - save locally\n",
    "      - return metadata rows + stats\n",
    "    \"\"\"\n",
    "    global y_auth, y_public\n",
    "    try:\n",
    "        if 'y_auth' not in globals() or y_auth is None:\n",
    "            y_auth = yadisk.Client(token=YA_TOKEN)\n",
    "        if 'y_public' not in globals() or y_public is None:\n",
    "            y_public = yadisk.Client()\n",
    "    except Exception:\n",
    "        # local-only runs will still work if src_mode=\"disk\"\n",
    "        pass\n",
    "\n",
    "    patient_folder, packed_files, max_files = args\n",
    "\n",
    "    processed_files = 0\n",
    "    processed_rows = 0\n",
    "    skipped_files = 0\n",
    "    errors = 0\n",
    "    rows_out: List[Dict[str, Any]] = []\n",
    "\n",
    "    for i, d in enumerate(packed_files):\n",
    "        sf = _unpack_source_file(d)\n",
    "        if max_files is not None and i >= max_files:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            dicom_bytes = download_source_bytes(sf, pc=yadisk.Client())\n",
    "            pngs, dicom_meta = dicom_to_png_bytes(dicom_bytes)\n",
    "\n",
    "            for frame_i, (suffix, png_bytes) in enumerate(pngs):\n",
    "                dest_path = dest_png_path_from_rel(sf.rel_path, frame_suffix=suffix)\n",
    "\n",
    "                if SKIP_IF_PNG_EXISTS:\n",
    "                    local_png = _local_png_path(dest_path)\n",
    "                    if os.path.exists(local_png):\n",
    "                        skipped_files += 1\n",
    "                        continue\n",
    "\n",
    "                save_png_locally(png_bytes, dest_path)\n",
    "\n",
    "                row = {\n",
    "                    \"processed_at\": _dt.datetime.utcnow().isoformat() + \"Z\",\n",
    "                    \"status\": \"OK\",\n",
    "                    \"error\": None,\n",
    "                    \"src_mode\": sf.src_mode,\n",
    "                    \"src_rel_path\": sf.rel_path,\n",
    "                    \"src_path\": sf.src_path,\n",
    "                    \"dest_png_path\": dest_path,\n",
    "                    \"frame_index\": frame_i if len(pngs) > 1 else 0,\n",
    "                    \"patient_folder\": patient_folder,\n",
    "                    **dicom_meta,\n",
    "                }\n",
    "                rows_out.append(row)\n",
    "                processed_rows += 1\n",
    "\n",
    "            processed_files += 1\n",
    "\n",
    "        except InvalidDicomError:\n",
    "            # treat as non-fatal skip (match your existing behavior if you do this)\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            rows_out.append({\n",
    "                \"processed_at\": _dt.datetime.utcnow().isoformat() + \"Z\",\n",
    "                \"status\": \"ERROR\",\n",
    "                \"error\": repr(e)[:2000],\n",
    "                \"src_mode\": sf.src_mode,\n",
    "                \"src_rel_path\": sf.rel_path,\n",
    "                \"src_path\": sf.src_path,\n",
    "                \"dest_png_path\": \"\",\n",
    "                \"frame_index\": \"\",\n",
    "                \"patient_folder\": patient_folder,\n",
    "            })\n",
    "\n",
    "    return (patient_folder, processed_files, processed_rows, skipped_files, errors, rows_out)\n",
    "\n",
    "\n",
    "def process_parallel(max_files: Optional[int] = None, workers: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Parallel version of process_all:\n",
    "      - respects PATIENTS_OFFSET / PATIENTS_LIMIT lazily\n",
    "      - processes patients in separate processes\n",
    "      - writes metadata.csv in main process (safe)\n",
    "    \"\"\"\n",
    "    workers = workers or PATIENTS_WORKERS\n",
    "\n",
    "    def _patient_args():\n",
    "        for idx, (patient, files) in enumerate(iter_source_files()):\n",
    "            if idx < PATIENTS_OFFSET:\n",
    "                continue\n",
    "            if PATIENTS_LIMIT is not None and idx >= PATIENTS_LIMIT:\n",
    "                break\n",
    "            packed_files = [_pack_source_file(sf) for sf in files]\n",
    "            yield (patient, packed_files, max_files)\n",
    "\n",
    "    processed_files = 0\n",
    "    processed_patients = 0\n",
    "    processed_rows = 0\n",
    "    skipped_files = 0\n",
    "    errors = 0\n",
    "\n",
    "    ex = get_reusable_executor(max_workers=workers)\n",
    "    in_flight = set()\n",
    "    pbar = tqdm(total=PATIENTS_LIMIT if PATIENTS_LIMIT is not None else None, desc=\"Patients\")\n",
    "    \n",
    "    def _drain_one():\n",
    "        nonlocal processed_patients, processed_files, processed_rows, skipped_files, errors\n",
    "        done = next(as_completed(in_flight))\n",
    "        in_flight.remove(done)\n",
    "\n",
    "        patient_folder, p_files, p_rows, p_skipped, p_errors, rows_out = done.result()\n",
    "        print(f\"processed patient: {patient_folder}\")\n",
    "        append_metadata_rows(METADATA_LOCAL_PATH, rows_out)\n",
    "\n",
    "        processed_patients += 1\n",
    "        processed_files += p_files\n",
    "        processed_rows += p_rows\n",
    "        skipped_files += p_skipped\n",
    "        errors += p_errors\n",
    "        pbar.update(1)\n",
    "\n",
    "    for args in _patient_args():\n",
    "        in_flight.add(ex.submit(_process_one_patient_worker, args))\n",
    "        if len(in_flight) >= workers * 2:   # bounded queue; adjust if you want\n",
    "            _drain_one()\n",
    "    while in_flight:\n",
    "        _drain_one()\n",
    "    \n",
    "    pbar.close()\n",
    "    upload_metadata_csv()\n",
    "\n",
    "    print('Done (parallel).')\n",
    "    print({\n",
    "        \"processed_files\": processed_files,\n",
    "        'processed patients': processed_patients,\n",
    "        \"processed_rows\": processed_rows,\n",
    "        \"skipped_files\": skipped_files,\n",
    "        \"errors\": errors,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9095b1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not pickle the task to send it to the workers.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/oovamoyo/PycharmProjects/brain_pd_et/.venv/lib/python3.13/site-packages/loky/backend/queues.py\", line 159, in _feed\n    obj_ = dumps(obj, reducers=reducers)\n  File \"/Users/oovamoyo/PycharmProjects/brain_pd_et/.venv/lib/python3.13/site-packages/loky/backend/reduction.py\", line 214, in dumps\n    dump(obj, buf, reducers=reducers, protocol=protocol)\n    ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/oovamoyo/PycharmProjects/brain_pd_et/.venv/lib/python3.13/site-packages/loky/backend/reduction.py\", line 207, in dump\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n  File \"/Users/oovamoyo/PycharmProjects/brain_pd_et/.venv/lib/python3.13/site-packages/cloudpickle/cloudpickle.py\", line 1313, in dump\n    return super().dump(obj)\n           ~~~~~~~~~~~~^^^^^\nTypeError: cannot pickle '_thread._local' object\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mPicklingError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 3.3 Usage\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mprocess_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 152\u001b[39m, in \u001b[36mprocess_parallel\u001b[39m\u001b[34m(max_files, workers)\u001b[39m\n\u001b[32m    150\u001b[39m         _drain_one()\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m in_flight:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[43m_drain_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m pbar.close()\n\u001b[32m    155\u001b[39m upload_metadata_csv()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 136\u001b[39m, in \u001b[36mprocess_parallel.<locals>._drain_one\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    133\u001b[39m done = \u001b[38;5;28mnext\u001b[39m(as_completed(in_flight))\n\u001b[32m    134\u001b[39m in_flight.remove(done)\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m patient_folder, p_files, p_rows, p_skipped, p_errors, rows_out = \u001b[43mdone\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprocessed patient: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatient_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    138\u001b[39m append_metadata_rows(METADATA_LOCAL_PATH, rows_out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mPicklingError\u001b[39m: Could not pickle the task to send it to the workers."
     ]
    }
   ],
   "source": [
    "# 3.3 Usage\n",
    "process_parallel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
